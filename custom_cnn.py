# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MNRg97pm7-0-HhsqzSOf9-cKR3HoUNGP
"""

# Cell 1: Install dependencies
!pip install -q tensorflow tensorflowjs matplotlib opencv-python pillow scikit-learn numpy pandas

# Cell 2: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Cell 3: Import libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, applications
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
import numpy as np
import cv2
import os
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import json
import shutil
from glob import glob
from tqdm import tqdm
import seaborn as sns
from sklearn.metrics import confusion_matrix
from PIL import Image

# Cell 4: Configuration
IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 50
LEARNING_RATE = 0.0001

CLASSES = [
    'Tomato___Bacterial_spot',
    'Tomato___Early_blight',
    'Tomato___Late_blight',
    'Tomato___Leaf_Mold',
    'Tomato___healthy'
]

SIMPLE_NAMES = {
    'Tomato___Bacterial_spot': 'Bacterial Spot',
    'Tomato___Early_blight': 'Early Blight',
    'Tomato___Late_blight': 'Late Blight',
    'Tomato___Leaf_Mold': 'Leaf Mold',
    'Tomato___healthy': 'Healthy'
}

# Cell 5: Load and preprocess dataset properly
def load_and_preprocess_dataset(base_path='/content/drive/MyDrive/PlantVillage', limit_per_class=500):
    images = []
    labels = []

    CLASS_INDICES = {class_name: idx for idx, class_name in enumerate(CLASSES)}

    print("üîç Looking for classes...")

    for class_name in CLASSES:
        class_path = os.path.join(base_path, class_name)

        if not os.path.exists(class_path):
            print(f"‚ö†Ô∏è  {class_name} not found at {class_path}")
            # Try alternative naming
            alt_paths = [
                os.path.join(base_path, class_name.replace('___', ' ')),
                os.path.join(base_path, class_name.replace('___', '_')),
            ]
            for alt_path in alt_paths:
                if os.path.exists(alt_path):
                    class_path = alt_path
                    print(f"   Found at alternative: {alt_path}")
                    break

        if not os.path.exists(class_path):
            print(f"‚ùå Could not find {class_name}")
            continue

        print(f"üìÇ Loading {class_name}...")

        # Get image files
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:
            image_files.extend(glob(os.path.join(class_path, ext)))

        print(f"   Found {len(image_files)} images")

        # Limit and shuffle
        np.random.shuffle(image_files)
        image_files = image_files[:min(limit_per_class, len(image_files))]

        # Load images
        loaded = 0
        for img_path in tqdm(image_files, desc=f"  Processing {class_name}"):
            try:
                # Load with PIL for better handling
                img = Image.open(img_path)
                img = img.convert('RGB')

                # Resize
                img = img.resize((IMG_SIZE, IMG_SIZE), Image.Resampling.LANCZOS)
                img_array = np.array(img)

                # Check if image is valid
                if img_array.shape != (IMG_SIZE, IMG_SIZE, 3):
                    continue

                # Normalize
                img_array = img_array.astype(np.float32) / 255.0

                images.append(img_array)
                labels.append(CLASS_INDICES[class_name])
                loaded += 1

            except Exception as e:
                continue

        print(f"   ‚úÖ Loaded {loaded} images")

    if len(images) == 0:
        print("‚ùå No images loaded!")
        return None, None

    images = np.array(images)
    labels = np.array(labels)

    print(f"\n‚úÖ Dataset loaded: {len(images)} images")

    # Show class distribution
    unique, counts = np.unique(labels, return_counts=True)
    print("\nüìä Class distribution:")
    for idx, count in zip(unique, counts):
        print(f"   {SIMPLE_NAMES[CLASSES[idx]]}: {count} images")

    return images, labels

# Load dataset
print("üöÄ Loading dataset from Google Drive...")
X, y = load_and_preprocess_dataset(limit_per_class=500)

if X is None:
    # Try alternative path
    print("\nüîÑ Trying alternative path...")
    X, y = load_and_preprocess_dataset(base_path='/content/drive/MyDrive', limit_per_class=500)

# Cell 6: Data preprocessing and augmentation
print("\nüîÑ Preprocessing data...")

if X is not None:
    # Split data
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )

    print(f"üìà Data split:")
    print(f"   Training: {X_train.shape[0]} images")
    print(f"   Validation: {X_val.shape[0]} images")
    print(f"   Test: {X_test.shape[0]} images")

    # Data augmentation for training
    train_datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.3,
        height_shift_range=0.3,
        shear_range=0.3,
        zoom_range=0.3,
        horizontal_flip=True,
        vertical_flip=True,
        brightness_range=[0.7, 1.3],
        channel_shift_range=0.2,
        fill_mode='nearest'
    )

    # For validation and test, only rescaling
    val_test_datagen = ImageDataGenerator()

    # Create generators
    train_generator = train_datagen.flow(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        shuffle=True
    )

    val_generator = val_test_datagen.flow(
        X_val, y_val,
        batch_size=BATCH_SIZE,
        shuffle=False
    )

    test_generator = val_test_datagen.flow(
        X_test, y_test,
        batch_size=BATCH_SIZE,
        shuffle=False
    )

else:
    print("‚ùå Failed to load dataset!")
    train_generator = val_generator = test_generator = None

# Cell 7: Create better model architecture
print("\nü§ñ Building enhanced model...")

def create_enhanced_model():
    # Use transfer learning with MobileNetV2 (pretrained on ImageNet)
    base_model = applications.MobileNetV2(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
        include_top=False,
        weights='imagenet',
        pooling='avg'
    )

    # Freeze base model layers
    base_model.trainable = False

    # Create new model on top
    model = models.Sequential([
        base_model,
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(len(CLASSES), activation='softmax')
    ])

    return model

# Create model
model = create_enhanced_model()  # Try transfer learning first

# Compile model
optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)
model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# Cell 8: Callbacks
print("\n‚è±Ô∏è Setting up callbacks...")

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1,
        min_delta=0.001
    ),

    ModelCheckpoint(
        'best_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),

    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    ),

    TensorBoard(
        log_dir='./logs',
        histogram_freq=1
    )
]

# Cell 9: Train model
print("\nüöÄ Starting training...")

if train_generator is not None:
    # First, train with frozen base
    print("üìö Phase 1: Training top layers...")
    history1 = model.fit(
        train_generator,
        steps_per_epoch=len(X_train) // BATCH_SIZE,
        validation_data=val_generator,
        validation_steps=len(X_val) // BATCH_SIZE,
        epochs=20,
        callbacks=callbacks,
        verbose=1
    )

    # Unfreeze some layers for fine-tuning
    print("\nüìö Phase 2: Fine-tuning...")
    model.layers[0].trainable = True

    # Only unfreeze last 50 layers
    for layer in model.layers[0].layers[:-50]:
        layer.trainable = False

    # Recompile with lower learning rate
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE/10),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    history2 = model.fit(
        train_generator,
        steps_per_epoch=len(X_train) // BATCH_SIZE,
        validation_data=val_generator,
        validation_steps=len(X_val) // BATCH_SIZE,
        epochs=30,
        callbacks=callbacks,
        verbose=1,
        initial_epoch=20
    )

    # Combine histories
    history = {
        'accuracy': history1.history['accuracy'] + history2.history['accuracy'],
        'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy'],
        'loss': history1.history['loss'] + history2.history['loss'],
        'val_loss': history1.history['val_loss'] + history2.history['val_loss']
    }

else:
    print("‚ùå Cannot train: No data available!")
    history = None

# Cell 10: Evaluate model
print("\nüìä Evaluating model...")

if os.path.exists('best_model.h5'):
    model = keras.models.load_model('best_model.h5')
    print("‚úÖ Loaded best model")
else:
    print("‚ö†Ô∏è Using last model")

if X_test is not None and len(X_test) > 0:
    # Evaluate on test set
    test_loss, test_acc = model.evaluate(test_generator, verbose=0)
    print(f"‚úÖ Test Accuracy: {test_acc:.4f}")
    print(f"‚úÖ Test Loss: {test_loss:.4f}")

    # Predictions
    y_pred = model.predict(test_generator, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = y_test

    # Classification report
    from sklearn.metrics import classification_report
    print("\nüìã Classification Report:")
    report = classification_report(y_true, y_pred_classes,
                                  target_names=[SIMPLE_NAMES[c] for c in CLASSES])
    print(report)

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred_classes)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[SIMPLE_NAMES[c] for c in CLASSES],
                yticklabels=[SIMPLE_NAMES[c] for c in CLASSES])
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=150)
    plt.show()

    # Plot training history
    if history:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        ax1.plot(history['accuracy'], label='Train Accuracy')
        ax1.plot(history['val_accuracy'], label='Val Accuracy')
        ax1.set_title('Model Accuracy')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.legend()
        ax1.grid(True)

        ax2.plot(history['loss'], label='Train Loss')
        ax2.plot(history['val_loss'], label='Val Loss')
        ax2.set_title('Model Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()
        plt.savefig('training_history.png', dpi=150)
        plt.show()

        # Show sample predictions
        print("\nüîç Sample predictions...")
        sample_indices = np.random.choice(len(X_test), min(6, len(X_test)), replace=False)

        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()

        for i, idx in enumerate(sample_indices):
            img = X_test[idx]
            true_label = y_true[idx]

            pred = model.predict(img[np.newaxis, ...], verbose=0)
            pred_class = np.argmax(pred)
            pred_prob = np.max(pred)

            axes[i].imshow(img)
            axes[i].set_title(f"True: {SIMPLE_NAMES[CLASSES[true_label]]}\n"
                             f"Pred: {SIMPLE_NAMES[CLASSES[pred_class]]}\n"
                             f"Conf: {pred_prob:.2%}")
            axes[i].axis('off')

        plt.tight_layout()
        plt.savefig('sample_predictions.png', dpi=150)
        plt.show()

