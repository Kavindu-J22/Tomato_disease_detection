# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NrT6y0dVymcJOeIrM2bAgOPhxVWSbUwL
"""

# Install TensorFlow.js converter
!pip install tensorflowjs

# Import libraries
import tensorflow as tf
import tensorflowjs as tfjs
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
import os
import zipfile

print("Libraries installed and imported successfully!")

from google.colab import drive
drive.mount('/content/drive')

# EXACT path from your screenshot
DATA_DIR = '/content/drive/MyDrive/PlantVillage'

# Image settings (Standard for MobileNetV2)
IMG_SIZE = (224, 224)
BATCH_SIZE = 32

# 1. Setup Data Augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,             # Normalize pixel values (0-1)
    rotation_range=25,          # Rotate images slightly
    width_shift_range=0.2,      # Shift width
    height_shift_range=0.2,     # Shift height
    shear_range=0.2,            # Shear transformation
    zoom_range=0.2,             # Zoom in/out
    horizontal_flip=True,       # Flip horizontally
    brightness_range=[0.8, 1.2],# Simulate different lighting (Crucial for mobile!)
    fill_mode='nearest',
    validation_split=0.2        # Use 20% of data for checking accuracy
)

# 2. Load Training Data (80%)
print("Loading Training Data:")
train_generator = train_datagen.flow_from_directory(
    DATA_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

# 3. Load Validation Data (20%)
print("\nLoading Validation Data:")
validation_generator = train_datagen.flow_from_directory(
    DATA_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

# IMPORTANT: Print the class mapping
# You MUST copy this output list for your React Native App
print("\nâœ… YOUR CLASS INDICES (Copy these for React Native):")
print(train_generator.class_indices)

# Load pre-trained MobileNetV2 (without the top "classification" layer)
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model (so we don't ruin the pre-learned patterns yet)
base_model.trainable = False

# Add our custom layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x) # Drops 30% of neurons randomly to prevent overfitting
predictions = Dense(train_generator.num_classes, activation='softmax')(x)

# Final Model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("Model built successfully!")

# 1. Re-compile the model to reset weights and start fresh
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 2. Train with the FIX (using len() to avoid the error)
epochs = 10

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),    # <--- The fix is here
    validation_data=validation_generator,
    validation_steps=len(validation_generator), # <--- And here
    epochs=epochs
)

# Unfreeze the top layers of the base model
base_model.trainable = True

# Fine-tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Recompile with a LOW learning rate (important!)
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              metrics=['accuracy'])

print("Starting Fine-Tuning...")
history_fine = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    epochs=5, # Train for 5 more epochs
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // BATCH_SIZE
)

model.save('tomato_model.h5')

tfjs.converters.save_keras_model(model, 'tfjs_model')

print("Conversion complete! Checking files...")
!ls tfjs_model

from google.colab import files

# 1. Zip the folder (browsers hate downloading 100 small files)
!zip -r tfjs_model.zip tfjs_model

# 2. Trigger the download
files.download('tfjs_model.zip')

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Re-load Validation Data (Shuffle=False is CRITICAL for the Confusion Matrix)
print("Preparing data for evaluation...")
eval_generator = train_datagen.flow_from_directory(
    DATA_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False  # <--- MUST BE FALSE for correct confusion matrix
)

# 2. Get Overall Accuracy
print("\nCalculating Overall Accuracy...")
loss, accuracy = model.evaluate(eval_generator)
print(f"\nðŸŒŸ Test Accuracy: {accuracy * 100:.2f}%")

# 3. Generate Predictions
print("Generating predictions (this might take a moment)...")
predictions = model.predict(eval_generator)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = eval_generator.classes
class_labels = list(eval_generator.class_indices.keys())

# 4. Create Confusion Matrix
cm = confusion_matrix(true_classes, predicted_classes)

# 5. Plot the Heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix: Where is the model making mistakes?')
plt.xticks(rotation=45, ha='right')
plt.show()

# 6. Detailed Report (Precision/Recall)
print("\nDetailed Classification Report:")
print(classification_report(true_classes, predicted_classes, target_names=class_labels))

